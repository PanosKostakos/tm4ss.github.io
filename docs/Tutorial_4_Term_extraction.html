<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Andreas Niekler, Gregor Wiedemann" />

<meta name="date" content="2019-07-12" />

<title>Tutorial 4: Key term extraction</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/united.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/clipboard-1.7.1/clipboard.min.js"></script>
<link href="site_libs/primer-tooltips-1.4.0/build.css" rel="stylesheet" />
<link href="site_libs/klippy-0.0.0.9500/css/klippy.min.css" rel="stylesheet" />
<script src="site_libs/klippy-0.0.0.9500/js/klippy.min.js"></script>
<script src="site_libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<link href="site_libs/wordcloud2-0.0.1/wordcloud.css" rel="stylesheet" />
<script src="site_libs/wordcloud2-0.0.1/wordcloud2-all.js"></script>
<script src="site_libs/wordcloud2-0.0.1/hover.js"></script>
<script src="site_libs/wordcloud2-binding-0.2.1/wordcloud2.js"></script>
<link href="site_libs/ionicons-2.0.1/css/ionicons.min.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->



<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html"></a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Intro</a>
</li>
<li>
  <a href="Tutorial_1_Read_textdata.html">
    <span class="ion ion-android-bulb"></span>
     
    Tutorial 1
  </a>
</li>
<li>
  <a href="Tutorial_2_Web_crawling.html">
    <span class="ion ion-android-bulb"></span>
     
    Tutorial 2
  </a>
</li>
<li>
  <a href="Tutorial_3_Frequency.html">
    <span class="ion ion-android-bulb"></span>
     
    Tutorial 3
  </a>
</li>
<li>
  <a href="Tutorial_4_Term_extraction.html">
    <span class="ion ion-android-bulb"></span>
     
    Tutorial 4
  </a>
</li>
<li>
  <a href="Tutorial_5_Co-occurrence.html">
    <span class="ion ion-android-bulb"></span>
     
    Tutorial 5
  </a>
</li>
<li>
  <a href="Tutorial_6_Topic_Models.html">
    <span class="ion ion-android-bulb"></span>
     
    Tutorial 6
  </a>
</li>
<li>
  <a href="Tutorial_7_Klassifikation.html">
    <span class="ion ion-android-bulb"></span>
     
    Tutorial 7
  </a>
</li>
<li>
  <a href="Tutorial_8_NER_POS.html">
    <span class="ion ion-android-bulb"></span>
     
    Tutorial 8
  </a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Tutorial 4: Key term extraction</h1>
<h4 class="author">Andreas Niekler, Gregor Wiedemann</h4>
<h4 class="date">2019-07-12</h4>

</div>


<p><script>
  addClassKlippyTo("pre.r, pre.markdown");
  addKlippy('left', 'top', 'auto', '1', 'Copy code', 'Copied!');
</script> This tutorial shows how to extract key terms from document and (sub-)collections with TF-IDF and the log-likelihood statistic and a reference corpus. We also show how it is possible to hande multi-word units such as `United States’ with the quanteda package.</p>
<ol style="list-style-type: decimal">
<li>Multi-word unit tokenization</li>
<li>TF-IDF</li>
<li>Log-likelihood ratio test</li>
<li>Visualization</li>
</ol>
<div id="multi-word-tokenization" class="section level1">
<h1><span class="header-section-number">1</span> Multi-word tokenization</h1>
<p>Like in the previous tutorial we read the CSV data file containing the State of the union addresses and preprocess the corpus object with a sequence of <code>quanteda</code> functions.</p>
<p>We also introduce handling of multi-word units (MWUs), also collocations in linguistics. MWUs are words comprising two or more tokens such as <code>machine learning', but also named entities such as</code>George Washington’. They can be inferred automatically with a statistical test. If two terms occur significantly more often as direct neighbors as expected by chance, they can be treated as collocations.</p>
<p>Quanteda provides two functions for handling MWUs: <code>textstat_collocations</code> performs a statsictical test to identify collocation candidates. <code>tokens_compound</code> concatenates collocation terms in a document with a separation character, e.g. <code>_</code>. By this, thw two terms are treated as a single new vocabulary type be each following text processing algorithm.</p>
<p>Finally, we create a Document-Term-Matrix as usual, but this time with unigram tokens together with MWU tokens.</p>
<pre class="r"><code>options(stringsAsFactors = FALSE)
library(quanteda)

# read the SOTU corpus data
textdata &lt;- read.csv(&quot;data/sotu.csv&quot;, sep = &quot;;&quot;, encoding = &quot;UTF-8&quot;)
corpus &lt;- corpus(textdata$text, docnames = textdata$doc_id)

# Build a dictionary of lemmas
lemma_data &lt;- read.csv(&quot;resources/baseform_en.tsv&quot;, encoding = &quot;UTF-8&quot;)

# read an extended stop word list
stopwords_extended &lt;- readLines(&quot;resources/stopwords_en.txt&quot;, encoding = &quot;UTF-8&quot;)

# Preprocessing of the corpus
corpus_tokens &lt;- corpus %&gt;% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %&gt;% 
  tokens_tolower() %&gt;% 
  tokens_replace(lemma_data$inflected_form, lemma_data$lemma, valuetype = &quot;fixed&quot;) %&gt;% 
  tokens_remove(pattern = stopwords_extended, padding = T)

# calculate multi-word unit candidates
sotu_collocations &lt;- textstat_collocations(corpus_tokens, min_count = 25)
# check top collocations
head(sotu_collocations, 25)</code></pre>
<pre><code>##                collocation count count_nested length   lambda         z
## 1              unite state  4494            0      2 8.399230 156.51776
## 2              fiscal year   768            0      2 7.588253  78.55422
## 3           annual message   204            0      2 7.822439  77.31989
## 4                 end june   223            0      2 6.955378  77.12319
## 5              health care   203            0      2 7.223379  76.89286
## 6       federal government   404            0      2 4.541596  75.91966
## 7              public debt   272            0      2 5.685086  74.99268
## 8          social security   196            0      2 7.090513  72.99111
## 9          american people   387            0      2 4.065779  72.21510
## 10               past year   303            0      2 4.952314  69.93989
## 11             public land   265            0      2 4.909891  69.86781
## 12                year end   315            0      2 4.656263  69.82836
## 13          billion dollar   156            0      2 7.317192  69.39719
## 14          million dollar   150            0      2 6.242481  63.69467
## 15                year ago   335            0      2 6.929959  60.30849
## 16            soviet union   124            0      2 7.115768  58.81716
## 17          fellow citizen   168            0      2 7.337374  57.76764
## 18             middle east   101            0      2 9.589199  55.96855
## 19         economic growth   105            0      2 6.278956  54.87529
## 20               arm force   123            0      2 5.692751  54.55016
## 21  commercial intercourse    90            0      2 6.778703  53.61324
## 22           supreme court   112            0      2 8.275154  53.47224
## 23     interstate commerce   107            0      2 7.536844  53.19561
## 24 favorable consideration    99            0      2 6.586640  53.15034
## 25         central america   107            0      2 6.591090  52.83145</code></pre>
<pre class="r"><code># check bottom collocations
tail(sotu_collocations, 25)</code></pre>
<pre><code>##                collocation count count_nested length     lambda         z
## 467         saddam hussein    27            0      2 16.5185544 11.161669
## 468           buenos ayres    31            0      2 16.2748623 11.133048
## 469               al qaeda    36            0      2 15.6530449 10.863418
## 470          good interest    33            0      2  1.8950314 10.846302
## 471            state court    29            0      2  2.0366064 10.838097
## 472             rio grande    51            0      2 15.4767636 10.813652
## 473          santo domingo    29            0      2 15.3924982 10.671724
## 474       state government   104            0      2  1.0126849 10.221307
## 475           make america    31            0      2  1.8314839 10.147610
## 476              good work    30            0      2  1.8330720 10.019031
## 477       congress provide    30            0      2  1.8272878  9.973710
## 478      ballistic missile    25            0      2 14.1327665  9.859976
## 479     government program    29            0      2  1.6961075  9.086864
## 480       state department    36            0      2  1.4759757  8.803264
## 481             great work    30            0      2  1.5895427  8.696619
## 482             bering sea    26            0      2 12.3504384  8.646594
## 483          present state    45            0      2  1.2851255  8.569483
## 484 government expenditure    25            0      2  1.6946336  8.443284
## 485            great power    29            0      2  1.4825933  7.982926
## 486       present congress    26            0      2  1.3014837  6.647607
## 487        american nation    25            0      2  1.2786034  6.410449
## 488          foreign state    25            0      2  1.1761402  5.883188
## 489              make good    30            0      2  1.0430855  5.718843
## 490         american state    36            0      2  0.7558060  4.532719
## 491    american government    30            0      2  0.7030012  3.857731</code></pre>
<p>Caution: For the calculation of collocation statistics being aware of deleted stop words, you need to add the paramter <code>padding = T</code> to the <code>tokens_remove</code> function above.</p>
<p>If you do not like all of the suggested collocation pairs to be considered as MWUs in the subsequent analysis, you can simply remove rows containing unwanted pairs from the sotu_collocations object.</p>
<pre class="r"><code># We will treat the top 250 collocations as MWU
sotu_collocations &lt;- sotu_collocations[1:250, ]

# compound collocations
corpus_tokens &lt;- tokens_compound(corpus_tokens, sotu_collocations)

# Create DTM (also remove padding empty term)
DTM &lt;- corpus_tokens %&gt;% 
  tokens_remove(&quot;&quot;) %&gt;%
  dfm() </code></pre>
</div>
<div id="tf-idf" class="section level1">
<h1><span class="header-section-number">2</span> TF-IDF</h1>
<p>A widely used method to weight terms according to their semantic contribution to a document is the <strong>term frequency–inverse document frequency</strong> measure (TF-IDF). The idea is, the more a term occurs in a document, the more contributing it is. At the same time, in the more documents a term occurs, the less informative it is for a single document. The product of both measures is the resulting weight.</p>
<p>Let us compute TF-IDF weights for all terms in the first speech of Barack Obama.</p>
<pre class="r"><code># Compute IDF: log(N / n_i)
number_of_docs &lt;- nrow(DTM)
term_in_docs &lt;- colSums(DTM &gt; 0)
idf &lt;- log2(number_of_docs / term_in_docs)

# Compute TF
first_obama_speech &lt;- which(textdata$president == &quot;Barack Obama&quot;)[1]
tf &lt;- as.vector(DTM[first_obama_speech, ])

# Compute TF-IDF
tf_idf &lt;- tf * idf
names(tf_idf) &lt;- colnames(DTM)</code></pre>
<p>The last operation is to append the column names again to the resulting term weight vector. If we now sort the tf-idf weights decreasingly, we get the most important terms for the Obama speech, according to this weight.</p>
<pre class="r"><code>sort(tf_idf, decreasing = T)[1:20]</code></pre>
<pre><code>## health_care    re-start         job     tonight        lend    recovery 
##    39.35091    31.40700    28.72608    24.28682    23.78066    22.20745 
##      layoff      ensure     college   renewable   recession      crisis 
##    20.55525    20.53077    19.93768    18.11928    16.33480    16.05375 
##      budget     inherit   long-term high_school accountable        iraq 
##    15.93825    15.40700    14.96518    14.48776    14.11928    14.11928 
##     quitter        auto 
##    13.70350    13.58946</code></pre>
<p>If we would have just relied upon term frequency, we would have obtained a list of stop words as most important terms. By re-weighting with inverse document frequency, we can see a heavy focus on business terms in the first speech. By the way, the quanteda-package provides a convenient function for computing tf-idf weights of a given DTM: <code>dfm_tfidf(DTM)</code>.</p>
</div>
<div id="log-likelihood" class="section level1">
<h1><span class="header-section-number">3</span> Log likelihood</h1>
<p>We now use a more sophisticated method with a comparison corpus and the log likelihood statistic.</p>
<pre class="r"><code>targetDTM &lt;- DTM

termCountsTarget &lt;- as.vector(targetDTM[first_obama_speech, ])
names(termCountsTarget) &lt;- colnames(targetDTM)
# Just keep counts greater than zero
termCountsTarget &lt;- termCountsTarget[termCountsTarget &gt; 0]</code></pre>
<p>In <em>termCountsTarget</em> we have the tf for the first Obama speech again.</p>
<p>As a comparison corpus, we select a corpus from the Leipzig Corpora Collection (<a href="http://corpora.uni-leipzig.de" class="uri">http://corpora.uni-leipzig.de</a>): 30.000 randomly selected sentences from the Wikipedia of 2010. <strong>CAUTION:</strong> The preprocessing of the comparison corpus must be identical to the preprocessing Of the target corpus to achieve meaningful results!</p>
<pre class="r"><code>lines &lt;- readLines(&quot;resources/eng_wikipedia_2010_30K-sentences.txt&quot;, encoding = &quot;UTF-8&quot;)
corpus_compare &lt;- corpus(lines)</code></pre>
<p>From the comparison corpus, we also create a count of all terms.</p>
<pre class="r"><code># Create a DTM (may take a while)
corpus_compare_edit &lt;- corpus_compare %&gt;% 
  tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %&gt;% 
  tokens_tolower() %&gt;% 
  tokens_replace(lemma_data$inflected_form, lemma_data$lemma, valuetype = &quot;fixed&quot;) %&gt;% 
  tokens_remove(pattern = stopwords_extended, padding = T)

# Create DTM
comparisonDTM &lt;- corpus_compare_edit %&gt;% 
  tokens_compound(sotu_collocations) %&gt;%
  tokens_remove(&quot;&quot;) %&gt;%
  dfm() 

termCountsComparison &lt;- colSums(comparisonDTM)</code></pre>
<p>In <em>termCountsComparison</em> we now have the frequencies of all (target) terms in the comparison corpus.</p>
<p>Let us now calculate the log-likelihood ratio test by comparing frequencies of a term in both corpora, taking the size of both corpora into account. First for a single term:</p>
<pre class="r"><code># Loglikelihood for a single term
term &lt;- &quot;health_care&quot;

# Determine variables
a &lt;- termCountsTarget[term]
b &lt;- termCountsComparison[term]
c &lt;- sum(termCountsTarget)
d &lt;- sum(termCountsComparison)

# Compute log likelihood test
Expected1 = c * (a+b) / (c+d)
Expected2 = d * (a+b) / (c+d)
t1 &lt;- a * log((a/Expected1))
t2 &lt;- b * log((b/Expected2))
logLikelihood &lt;- 2 * (t1 + t2)

print(logLikelihood)</code></pre>
<pre><code>## health_care 
##    121.1461</code></pre>
<p>The LL value indicates whether the term occurs significantly more frequently / less frequently in the target counts than we would expect from the observation in the comparative counts. Specific significance thresholds are defined for the LL values:</p>
<ul>
<li>95th percentile; 5% level; p &lt; 0.05; critical value = 3.84</li>
<li>99th percentile; 1% level; p &lt; 0.01; critical value = 6.63</li>
<li>99.9th percentile; 0.1% level; p &lt; 0.001; critical value = 10.83</li>
<li>99.99th percentile; 0.01% level; p &lt; 0.0001; critical value = 15.13</li>
</ul>
<p>With R it is easy to calculate the LL-value for all terms at once. This is possible because many computing operations in R can be applied not only to individual values, but to entire vectors and matrices. For example, <code>a / 2</code> results in a single value <em>a divided by 2</em> if <code>a</code> is a single number. If <code>a</code> is a vector, the result is also a vector, in which all values are divided by 2.</p>
<p>ATTENTION: A comparison of term occurrences between two documents/corpora is actually only useful if the term occurs in both units. Since, however, we also want to include terms which are not contained in the comparative corpus (the <code>termCountsComparison</code> vector contains 0 values for these terms), we simply add 1 to all counts during the test. This is necessary to avoid <code>NaN</code> values which otherwise would result from the log-function on 0-values during the LL test. Alternatively, the test could be performed only on terms that actually occur in both corpora.</p>
<p>First, let’s have a look into the set of terms only occurring in the target document, but not in the comparison corpus.</p>
<pre class="r"><code># use set operation to get terms only occurring in target document
uniqueTerms &lt;- setdiff(names(termCountsTarget), names(termCountsComparison))
# Have a look into a random selection of terms unique in the target corpus
sample(uniqueTerms, 20)</code></pre>
<pre><code>##  [1] &quot;sleepless&quot;                  &quot;cyber&quot;                     
##  [3] &quot;down-payment&quot;               &quot;apprenticeship&quot;            
##  [5] &quot;re-finances&quot;                &quot;jumpstart&quot;                 
##  [7] &quot;plug-in&quot;                    &quot;common-sense&quot;              
##  [9] &quot;re-start&quot;                   &quot;candidly&quot;                  
## [11] &quot;speaker_mr_versa_president&quot; &quot;peril&quot;                     
## [13] &quot;war-era&quot;                    &quot;orrin&quot;                     
## [15] &quot;re-tooled&quot;                  &quot;candor&quot;                    
## [17] &quot;no-bid&quot;                     &quot;bethea&quot;                    
## [19] &quot;hardest-working&quot;            &quot;decency&quot;</code></pre>
<p>Now we calculate the statistics the same way as above, but with vectors. But, since there might be terms in the targetCounts which we did not observe in the comparison corpus, we need to make both vocabularies matching. For this, we append unique terms from the target as zero counts to the comparison frequency vector.</p>
<p>Moreover, we use a little trick to check for zero counts of frequency values in a or b when computing t1 or t2. If a count is zero the log function would produce an NaN value, which we want to avoid. In this case the <code>a == 0</code> resp. <code>b == 0</code> expression add 1 to the expression which yields a 0 value after applying the log function.</p>
<pre class="r"><code># Create vector of zeros to append to comparison counts
zeroCounts &lt;- rep(0, length(uniqueTerms))
names(zeroCounts) &lt;- uniqueTerms
termCountsComparison &lt;- c(termCountsComparison, zeroCounts)

# Get list of terms to compare from intersection of target and comparison vocabulary
termsToCompare &lt;- intersect(names(termCountsTarget), names(termCountsComparison))

# Calculate statistics (same as above, but now with vectors!)
a &lt;- termCountsTarget[termsToCompare]
b &lt;- termCountsComparison[termsToCompare]
c &lt;- sum(termCountsTarget)
d &lt;- sum(termCountsComparison)
Expected1 = c * (a+b) / (c+d)
Expected2 = d * (a+b) / (c+d)
t1 &lt;- a * log((a/Expected1) + (a == 0))
t2 &lt;- b * log((b/Expected2) + (b == 0))
logLikelihood &lt;- 2 * (t1 + t2)

# Compare relative frequencies to indicate over/underuse
relA &lt;- a / c
relB &lt;- b / d
# underused terms are multiplied by -1
logLikelihood[relA &lt; relB] &lt;- logLikelihood[relA &lt; relB] * -1</code></pre>
<p>Let’s take a look at the results: The 50 more frequently used / less frequently used terms, and then the more frequently used terms compared to their frequency. We also see terms that have comparatively low frequencies are identified by the LL test as statistically significant compared to the reference corpus.</p>
<pre class="r"><code># top terms (overuse in targetCorpus compared to comparisonCorpus)
sort(logLikelihood, decreasing=TRUE)[1:50]</code></pre>
<pre><code>##     health_care        american         economy             job 
##       121.14613       110.85756       101.27230        87.63369 
##         tonight         america          budget        recovery 
##        85.01941        67.90865        67.60581        66.14296 
##          crisis            lend         deficit            plan 
##        65.33199        62.73237        57.98055        55.26975 
##          reform            cost  responsibility          nation 
##        54.97901        53.80704        53.07413        51.05914 
##        congress          energy       education          afford 
##        48.28926        45.79050        42.83649        42.39513 
##       recession american_people      confidence            bank 
##        41.82158        40.29198        40.07235        39.42911 
##     accountable        re-start       long-term          invest 
##        38.90939        38.90939        36.46019        34.86676 
##            loan          ensure         tax_cut          dollar 
##        34.38881        34.17352        33.92087        33.49738 
##      prosperity            debt        medicare             bad 
##        31.43753        30.58356        29.18204        28.97788 
##         country          future        taxpayer       renewable 
##        27.79142        25.61839        25.54218        25.54218 
##           money             buy          layoff           spend 
##        25.36427        24.92780        24.69887        23.04517 
##         college        business        economic         inherit 
##        22.27778        21.89767        20.63075        20.56073 
##       financial      investment 
##        20.19135        20.10559</code></pre>
<pre class="r"><code># bottom terms (underuse in targetCorpus compared to comparisonCorpus)
sort(logLikelihood, decreasing=FALSE)[1:25]</code></pre>
<pre><code>##       game       city     follow      early        win       numb 
## -3.7519628 -3.5736548 -2.5277954 -2.4613495 -1.8702817 -1.8079878 
##      state      point      leave       show       book     record 
## -1.6920659 -1.6543846 -1.5823090 -1.2507745 -1.1028102 -1.0663127 
##       area    include university       type     design    control 
## -1.0165030 -1.0073460 -0.8205736 -0.7869771 -0.7786415 -0.6410402 
##        age        run      local      fight    produce    general 
## -0.4614764 -0.4586085 -0.4403883 -0.4196276 -0.3992028 -0.3924707 
##    attempt 
## -0.3529017</code></pre>
<pre class="r"><code>llTop100 &lt;- sort(logLikelihood, decreasing=TRUE)[1:100]
frqTop100 &lt;- termCountsTarget[names(llTop100)]
frqLLcomparison &lt;- data.frame(llTop100, frqTop100)
View(frqLLcomparison)

# Number of signficantly overused terms (p &lt; 0.01)
sum(logLikelihood &gt; 6.63)</code></pre>
<pre><code>## [1] 268</code></pre>
<p>The method extracted 268 key terms from the first Obama speech.</p>
</div>
<div id="visualization" class="section level1">
<h1><span class="header-section-number">4</span> Visualization</h1>
<p>Finally, visualize the result of the 50 most significant terms as Wordcloud. This can be realized simply by function of the package wordcloud. Additionally to the words and their weights (here we use likelihood values), we override default scaling and color parameters. Feel free to try different parameters to modify the wordcloud rendering.</p>
<pre class="r"><code>require(wordcloud2)
top50 &lt;- sort(logLikelihood, decreasing = TRUE)[1:50]
top50_df &lt;- data.frame(word = names(top50), count = top50)
wordcloud2(top50_df, shuffle = F)</code></pre>
<div id="htmlwidget-0bb3d584c1b625ce4664" style="width:480px;height:480px;" class="wordcloud2 html-widget"></div>
<script type="application/json" data-for="htmlwidget-0bb3d584c1b625ce4664">{"x":{"word":["health_care","american","economy","job","tonight","america","budget","recovery","crisis","lend","deficit","plan","reform","cost","responsibility","nation","congress","energy","education","afford","recession","american_people","confidence","bank","accountable","re-start","long-term","invest","loan","ensure","tax_cut","dollar","prosperity","debt","medicare","bad","country","future","taxpayer","renewable","money","buy","layoff","spend","college","business","economic","inherit","financial","investment"],"freq":[121.14613415177,110.857555373522,101.2723011905,87.6336901812216,85.0194114288819,67.9086511188169,67.60580538979,66.1429569785249,65.3319853227863,62.7323725982268,57.9805450706087,55.2697526647995,54.9790092638929,53.8070382531255,53.0741345806867,51.0591437642481,48.2892621818513,45.7904975741416,42.8364899053643,42.3951251993764,41.8215817321512,40.2919753056779,40.0723502019216,39.4291103364368,38.9093931431221,38.9093931431221,36.4601902924017,34.866758191393,34.3888065281635,34.1735208797188,33.9208729662246,33.4973819903555,31.4375343939967,30.5835554801517,29.1820448573415,28.9778836292178,27.7914167720267,25.61838629094,25.5421841538437,25.5421841538437,25.364272254934,24.9277985155009,24.6988677588755,23.0451743379437,22.2777762205329,21.8976730078428,20.6307536679189,20.5607303884679,20.1913520658706,20.1055894755012],"fontFamily":"Segoe UI","fontWeight":"bold","color":"random-dark","minSize":0,"weightFactor":1.4858088643134,"backgroundColor":"white","gridSize":0,"minRotation":-0.785398163397448,"maxRotation":0.785398163397448,"shuffle":false,"rotateRatio":0.4,"shape":"circle","ellipticity":0.65,"figBase64":null,"hover":null},"evals":[],"jsHooks":{"render":[{"code":"function(el,x){\n                        console.log(123);\n                        if(!iii){\n                          window.location.reload();\n                          iii = False;\n\n                        }\n  }","data":null}]}}</script>
<p>Key term extraction cannot be done for single documents, but for entire (sub-)corpora. Depending on the comparison corpora, the results may vary. Instead of comparing a single document to a Wikipedia corpus, we now compare collections of speeches of a single president, to speeches of all other presidents.</p>
<p>For this, we iterate over all different president names using a for-loop. Within the loop, we utilize a logical vector (Boolean TRUE/FALSE values), to split the DTM into two sub matrices: rows of the currently selected president and rows of all other presidents. From these matrices our counts of target and comparison frequencies are created. The statistical computation of the log-likelihood measure from above, we outsourced into the function <code>calculateLogLikelihood</code> which we load with the <code>source</code> command at the beginning of the block. The function just takes both frequency vectors as input parameters and outputs a LL-value for each term of the target vector.</p>
<p>Results of the LL key term extraction are visualized again as a wordcloud. Instead of plotting the wordcloud into RStudio, this time we write the visualization as a PDF-file to disk into the <code>wordclouds</code> folder. After the for-loop is completed, the folder should contain 42 wordcloud PDFs, one for each president.</p>
<pre class="r"><code>source(&quot;calculateLogLikelihood.R&quot;)

presidents &lt;- unique(textdata$president)
for (president in presidents) {
  
  cat(&quot;Extracting terms for president&quot;, president, &quot;\n&quot;)
  
  selector_logical_idx &lt;- textdata$president == president
  
  presidentDTM &lt;- targetDTM[selector_logical_idx, ]
  termCountsTarget &lt;- colSums(presidentDTM)
  
  otherDTM &lt;- targetDTM[!selector_logical_idx, ]
  termCountsComparison &lt;- colSums(otherDTM)
  
  loglik_terms &lt;- calculateLogLikelihood(termCountsTarget, termCountsComparison)
  
  top50 &lt;- sort(loglik_terms, decreasing = TRUE)[1:50]
  
  fileName &lt;- paste0(&quot;wordclouds/&quot;, president, &quot;.pdf&quot;)
  pdf(fileName, width = 9, height = 7)
  wordcloud::wordcloud(names(top50), top50, max.words = 50, scale = c(3, .9), colors = RColorBrewer::brewer.pal(8, &quot;Dark2&quot;), random.order = F)
  dev.off()
  
}</code></pre>
</div>
<div id="optional-exercises" class="section level1">
<h1><span class="header-section-number">5</span> Optional exercises</h1>
<ol style="list-style-type: decimal">
<li>Create a table (data.frame), which displays the top 25 terms of all speeches by frequency, tf-idf and log likelihood in columns.</li>
</ol>
<pre><code>##       word.frq  frq word.tfidf     tfidf     word.ll        ll
## 1   government 6588    program 1451.8011    congress 3072.5270
## 2         make 5844    tonight 1167.9753  government 2735.0318
## 3     congress 5013        job 1093.1030 unite_state 2010.0529
## 4  unite_state 4494     mexico  979.7642      nation 1675.5660
## 5        state 4302    america  875.7261     country 1494.8079
## 6      country 4247  territory  802.1587         law 1065.9834
## 7         year 4078   economic  784.9759       peace  960.0296
## 8       people 3736       bank  763.7570        duty  917.3458
## 9        great 3532       cent  745.5283       great  908.2986
## 10      nation 3299     budget  737.1438    interest  898.1826</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Create a wordcloud which compares Obama’s last speech with all his other speeches. <img src="Tutorial_4_Term_extraction_files/figure-html/extra2-1.png" width="672" /></li>
</ol>
</div>

<p>2019, Andreas Niekler and Gregor Wiedemann. GPLv3. <a href="https://tm4ss.github.io">tm4ss.github.io</a></p>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
